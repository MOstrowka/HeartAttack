{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1151b0d3-21a0-4503-86bb-193fc32bef51",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "184d3026-cef0-4e42-8e2d-fcf61b99e5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53fc1c6-d72f-4054-8fbf-669655c8fffa",
   "metadata": {},
   "source": [
    "## Preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57e0f57e-9952-43bb-a19f-e6c2131ae321",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_path = 'Data/X_preprocessed.csv'\n",
    "y_path = 'Data/y_preprocessed.csv'\n",
    "\n",
    "X = pd.read_csv(X_path)\n",
    "y = pd.read_csv(y_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34365b7-9914-4626-84db-3c2868efd169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a DataFrame to store metrics for different models\n",
    "metrics_df = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'])\n",
    "\n",
    "def evaluate_sklearn_model(model, X, y, params=None, cv=5):\n",
    "    \"\"\"\n",
    "    Function to perform cross-validation with a specified model, display the confusion matrix, \n",
    "    plot the ROC AUC curve, and calculate the model's accuracy. Optionally performs hyperparameter \n",
    "    tuning using GridSearchCV. Also, collects important metrics for further comparison.\n",
    "    \n",
    "    :param model: The classification model (e.g., LogisticRegression, RandomForestClassifier).\n",
    "    :param X: Feature dataset.\n",
    "    :param y: Target labels.\n",
    "    :param params: Parameter grid for GridSearchCV (optional).\n",
    "    :param cv: Number of cross-validation folds (default is 5).\n",
    "    :return: The best model and its accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If parameter grid is provided, perform GridSearchCV\n",
    "    if params:\n",
    "        grid_search = GridSearchCV(model, param_grid=params, cv=cv, scoring='accuracy', verbose=1)\n",
    "        grid_search.fit(X, y)\n",
    "        model = grid_search.best_estimator_  # Update model to the best found by GridSearchCV\n",
    "        print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Predict values using cross-validation\n",
    "    y_pred = cross_val_predict(model, X, y, cv=cv, method='predict')\n",
    "    y_pred_proba = cross_val_predict(model, X, y, cv=cv, method='predict_proba')[:, 1]\n",
    "\n",
    "    # Compute the confusion matrix\n",
    "    conf = confusion_matrix(y, y_pred)\n",
    "\n",
    "    # Create a figure with two subplots (1 row, 2 columns)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Plot the confusion matrix on the first subplot\n",
    "    sns.heatmap(conf, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "    axes[0].set_title(f'Confusion Matrix - {cv}-Fold Cross-Validation')\n",
    "    axes[0].set_xlabel('Predicted')\n",
    "    axes[0].set_ylabel('Actual')\n",
    "\n",
    "    # Calculate ROC AUC\n",
    "    roc_auc = roc_auc_score(y, y_pred_proba)\n",
    "\n",
    "    # Plot the ROC AUC curve on the second subplot\n",
    "    fpr, tpr, _ = roc_curve(y, y_pred_proba)\n",
    "    axes[1].plot(fpr, tpr, label=f'ROC AUC = {roc_auc:.2f}')\n",
    "    axes[1].plot([0, 1], [0, 1], linestyle='--')\n",
    "    axes[1].set_xlabel('False Positive Rate')\n",
    "    axes[1].set_ylabel('True Positive Rate')\n",
    "    axes[1].set_title(f'ROC AUC Curve - {cv}-Fold Cross-Validation')\n",
    "    axes[1].legend(loc='lower right')\n",
    "\n",
    "    # Ensure the 'Results' directory exists\n",
    "    os.makedirs('Results', exist_ok=True)\n",
    "\n",
    "    # Save the figure to the 'Results' directory with a dynamic name based on the model\n",
    "    model_name = model.__class__.__name__\n",
    "    plot_path = os.path.join('Results', f'results_{model_name}.png')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_path)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred)\n",
    "    recall = recall_score(y, y_pred)\n",
    "    f1 = f1_score(y, y_pred)\n",
    "\n",
    "    # Append the metrics to the DataFrame\n",
    "    metrics_df.loc[len(metrics_df)] = [model_name, accuracy, precision, recall, f1, roc_auc]\n",
    "    \n",
    "    return model, accuracy, metrics_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
